---
title: "ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–"
description: "æ·±å…¥å­¦ä¹ æ–°èƒ½æºç³»ç»Ÿçš„æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬ç®—æ³•ä¼˜åŒ–ã€å†…å­˜ç®¡ç†ã€å¹¶å‘å¤„ç†å’Œç³»ç»Ÿè°ƒä¼˜ã€‚"
author: "æ–°èƒ½æºç¼–ç¨‹ä¿±ä¹éƒ¨"
date: "2024-01-15"
tags: ["æ€§èƒ½ä¼˜åŒ–", "ç®—æ³•ä¼˜åŒ–", "ç³»ç»Ÿè°ƒä¼˜", "å¹¶å‘ç¼–ç¨‹"]
category: "tutorials"
subcategory: "advanced"
slug: "optimization"
order: 1
toc: true
estimated_time: "180åˆ†é’Ÿ"
difficulty: "advanced"
prerequisites: ["ç¼–ç¨‹åŸºç¡€", "æ•°æ®ç»“æ„", "æ“ä½œç³»ç»ŸåŸç†"]
---

# ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–

åœ¨æ–°èƒ½æºç³»ç»Ÿå¼€å‘ä¸­ï¼Œæ€§èƒ½ä¼˜åŒ–æ˜¯ç¡®ä¿ç³»ç»Ÿç¨³å®šè¿è¡Œå’Œé«˜æ•ˆå¤„ç†çš„å…³é”®æŠ€æœ¯ã€‚æœ¬æ•™ç¨‹å°†æ·±å…¥æ¢è®¨å„ç§ä¼˜åŒ–æŠ€æœ¯å’Œæœ€ä½³å®è·µã€‚

## â±ï¸ å­¦ä¹ æ—¶é—´
**180åˆ†é’Ÿ** | éš¾åº¦ï¼šâ­â­â­â­â­

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ•™ç¨‹åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- [ ] ç†è§£æ€§èƒ½ä¼˜åŒ–çš„åŸºæœ¬åŸç†å’Œæ–¹æ³•è®º
- [ ] æŒæ¡ç®—æ³•å’Œæ•°æ®ç»“æ„ä¼˜åŒ–æŠ€æœ¯
- [ ] å®ç°é«˜æ•ˆçš„å†…å­˜ç®¡ç†ç­–ç•¥
- [ ] åº”ç”¨å¹¶å‘å’Œå¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯
- [ ] è¿›è¡Œç³»ç»Ÿçº§æ€§èƒ½è°ƒä¼˜
- [ ] ä½¿ç”¨æ€§èƒ½åˆ†æå’Œç›‘æ§å·¥å…·
- [ ] ä¼˜åŒ–æ–°èƒ½æºæ•°æ®å¤„ç†ç³»ç»Ÿ

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–æ¦‚è¿°

### ä¼˜åŒ–å±‚æ¬¡

```
åº”ç”¨å±‚ä¼˜åŒ–
â”œâ”€â”€ ç®—æ³•ä¼˜åŒ–
â”œâ”€â”€ æ•°æ®ç»“æ„ä¼˜åŒ–
â”œâ”€â”€ ä»£ç ä¼˜åŒ–
â””â”€â”€ æ¶æ„ä¼˜åŒ–

ç³»ç»Ÿå±‚ä¼˜åŒ–
â”œâ”€â”€ å†…å­˜ç®¡ç†
â”œâ”€â”€ I/Oä¼˜åŒ–
â”œâ”€â”€ å¹¶å‘å¤„ç†
â””â”€â”€ ç¼“å­˜ç­–ç•¥

ç¡¬ä»¶å±‚ä¼˜åŒ–
â”œâ”€â”€ CPUä¼˜åŒ–
â”œâ”€â”€ å†…å­˜ä¼˜åŒ–
â”œâ”€â”€ å­˜å‚¨ä¼˜åŒ–
â””â”€â”€ ç½‘ç»œä¼˜åŒ–
```

### æ€§èƒ½æŒ‡æ ‡

- **å“åº”æ—¶é—´**ï¼šç³»ç»Ÿå¤„ç†è¯·æ±‚çš„æ—¶é—´
- **ååé‡**ï¼šå•ä½æ—¶é—´å†…å¤„ç†çš„è¯·æ±‚æ•°é‡
- **èµ„æºåˆ©ç”¨ç‡**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œçš„ä½¿ç”¨æ•ˆç‡
- **å¹¶å‘èƒ½åŠ›**ï¼šåŒæ—¶å¤„ç†å¤šä¸ªä»»åŠ¡çš„èƒ½åŠ›
- **å¯æ‰©å±•æ€§**ï¼šç³»ç»Ÿå¤„ç†è´Ÿè½½å¢é•¿çš„èƒ½åŠ›

## ğŸ§® ç®—æ³•ä¼˜åŒ–

### æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–

#### ç¤ºä¾‹ï¼šå¤ªé˜³èƒ½åŠŸç‡é¢„æµ‹ç®—æ³•ä¼˜åŒ–

**åŸå§‹å®ç°ï¼ˆO(nÂ²)ï¼‰**ï¼š
```python
def predict_solar_power_naive(historical_data, weather_data):
    """
    æœ´ç´ çš„å¤ªé˜³èƒ½åŠŸç‡é¢„æµ‹ç®—æ³•
    æ—¶é—´å¤æ‚åº¦ï¼šO(nÂ²)
    """
    predictions = []
    
    for i, current_weather in enumerate(weather_data):
        # å¯¹æ¯ä¸ªé¢„æµ‹ç‚¹ï¼Œéå†æ‰€æœ‰å†å²æ•°æ®å¯»æ‰¾ç›¸ä¼¼å¤©æ°”
        similar_powers = []
        
        for j, historical_point in enumerate(historical_data):
            # è®¡ç®—å¤©æ°”ç›¸ä¼¼åº¦
            similarity = calculate_weather_similarity(
                current_weather, 
                historical_point['weather']
            )
            
            if similarity > 0.8:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                similar_powers.append(historical_point['power'])
        
        # é¢„æµ‹åŠŸç‡ä¸ºç›¸ä¼¼å¤©æ°”ä¸‹çš„å¹³å‡åŠŸç‡
        if similar_powers:
            predicted_power = sum(similar_powers) / len(similar_powers)
        else:
            predicted_power = 0
            
        predictions.append(predicted_power)
    
    return predictions

def calculate_weather_similarity(weather1, weather2):
    """
    è®¡ç®—å¤©æ°”ç›¸ä¼¼åº¦
    """
    # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
    temp_diff = abs(weather1['temperature'] - weather2['temperature'])
    humidity_diff = abs(weather1['humidity'] - weather2['humidity'])
    cloud_diff = abs(weather1['cloud_cover'] - weather2['cloud_cover'])
    
    # å½’ä¸€åŒ–ç›¸ä¼¼åº¦
    similarity = 1 - (temp_diff / 50 + humidity_diff / 100 + cloud_diff / 100) / 3
    return max(0, similarity)
```

**ä¼˜åŒ–å®ç°ï¼ˆO(n log n)ï¼‰**ï¼š
```python
import numpy as np
from sklearn.neighbors import KDTree
from sklearn.preprocessing import StandardScaler

class OptimizedSolarPredictor:
    def __init__(self):
        self.kdtree = None
        self.scaler = StandardScaler()
        self.historical_powers = None
        
    def fit(self, historical_data):
        """
        è®­ç»ƒé¢„æµ‹æ¨¡å‹
        æ—¶é—´å¤æ‚åº¦ï¼šO(n log n)
        """
        # æå–å¤©æ°”ç‰¹å¾
        weather_features = np.array([
            [point['weather']['temperature'],
             point['weather']['humidity'],
             point['weather']['cloud_cover'],
             point['weather']['wind_speed']]
            for point in historical_data
        ])
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        weather_features_scaled = self.scaler.fit_transform(weather_features)
        
        # æ„å»ºKDæ ‘ç”¨äºå¿«é€Ÿæœ€è¿‘é‚»æœç´¢
        self.kdtree = KDTree(weather_features_scaled)
        
        # ä¿å­˜å¯¹åº”çš„åŠŸç‡å€¼
        self.historical_powers = np.array([
            point['power'] for point in historical_data
        ])
        
    def predict(self, weather_data, k=5):
        """
        é¢„æµ‹å¤ªé˜³èƒ½åŠŸç‡
        æ—¶é—´å¤æ‚åº¦ï¼šO(m log n)ï¼Œå…¶ä¸­mæ˜¯é¢„æµ‹ç‚¹æ•°ï¼Œnæ˜¯å†å²æ•°æ®ç‚¹æ•°
        """
        # æå–å¤©æ°”ç‰¹å¾
        weather_features = np.array([
            [weather['temperature'],
             weather['humidity'],
             weather['cloud_cover'],
             weather['wind_speed']]
            for weather in weather_data
        ])
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        weather_features_scaled = self.scaler.transform(weather_features)
        
        # æŸ¥æ‰¾kä¸ªæœ€è¿‘é‚»
        distances, indices = self.kdtree.query(
            weather_features_scaled, k=k
        )
        
        predictions = []
        for i in range(len(weather_data)):
            # åŸºäºè·ç¦»çš„åŠ æƒå¹³å‡
            weights = 1 / (distances[i] + 1e-8)  # é¿å…é™¤é›¶
            weights = weights / np.sum(weights)  # å½’ä¸€åŒ–æƒé‡
            
            # åŠ æƒå¹³å‡é¢„æµ‹
            predicted_power = np.sum(
                self.historical_powers[indices[i]] * weights
            )
            predictions.append(predicted_power)
            
        return predictions

# ä½¿ç”¨ç¤ºä¾‹
predictor = OptimizedSolarPredictor()
predictor.fit(historical_data)
predictions = predictor.predict(weather_data)
```

### ç©ºé—´å¤æ‚åº¦ä¼˜åŒ–

#### æµå¼æ•°æ®å¤„ç†

```python
class StreamingSolarDataProcessor:
    """
    æµå¼å¤„ç†å¤ªé˜³èƒ½æ•°æ®ï¼Œå‡å°‘å†…å­˜å ç”¨
    """
    def __init__(self, window_size=1000):
        self.window_size = window_size
        self.data_buffer = []
        self.statistics = {
            'count': 0,
            'sum': 0,
            'sum_squares': 0,
            'min': float('inf'),
            'max': float('-inf')
        }
        
    def process_data_point(self, power_value):
        """
        å¤„ç†å•ä¸ªæ•°æ®ç‚¹ï¼Œç»´æŒå›ºå®šå¤§å°çš„æ»‘åŠ¨çª—å£
        ç©ºé—´å¤æ‚åº¦ï¼šO(window_size)
        """
        # æ·»åŠ æ–°æ•°æ®ç‚¹
        self.data_buffer.append(power_value)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.statistics['count'] += 1
        self.statistics['sum'] += power_value
        self.statistics['sum_squares'] += power_value ** 2
        self.statistics['min'] = min(self.statistics['min'], power_value)
        self.statistics['max'] = max(self.statistics['max'], power_value)
        
        # ç»´æŒçª—å£å¤§å°
        if len(self.data_buffer) > self.window_size:
            old_value = self.data_buffer.pop(0)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯ï¼ˆç§»é™¤æ—§å€¼çš„å½±å“ï¼‰
            self.statistics['sum'] -= old_value
            self.statistics['sum_squares'] -= old_value ** 2
            
            # é‡æ–°è®¡ç®—minå’Œmaxï¼ˆå¦‚æœéœ€è¦ï¼‰
            if old_value == self.statistics['min'] or old_value == self.statistics['max']:
                self.statistics['min'] = min(self.data_buffer)
                self.statistics['max'] = max(self.data_buffer)
                
    def get_statistics(self):
        """
        è·å–å½“å‰çª—å£çš„ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.data_buffer:
            return None
            
        n = len(self.data_buffer)
        mean = self.statistics['sum'] / n
        variance = (self.statistics['sum_squares'] / n) - (mean ** 2)
        std_dev = variance ** 0.5
        
        return {
            'count': n,
            'mean': mean,
            'std_dev': std_dev,
            'min': self.statistics['min'],
            'max': self.statistics['max'],
            'current_window': self.data_buffer.copy()
        }
```

## ğŸ§  å†…å­˜ç®¡ç†ä¼˜åŒ–

### å¯¹è±¡æ± æ¨¡å¼

```python
class SensorDataPool:
    """
    ä¼ æ„Ÿå™¨æ•°æ®å¯¹è±¡æ± ï¼Œå‡å°‘é¢‘ç¹çš„å¯¹è±¡åˆ›å»ºå’Œé”€æ¯
    """
    def __init__(self, initial_size=100):
        self.available_objects = []
        self.in_use_objects = set()
        
        # é¢„åˆ›å»ºå¯¹è±¡
        for _ in range(initial_size):
            self.available_objects.append(SensorData())
            
    def acquire(self):
        """
        è·å–ä¸€ä¸ªå¯¹è±¡
        """
        if self.available_objects:
            obj = self.available_objects.pop()
        else:
            # æ± ä¸­æ²¡æœ‰å¯ç”¨å¯¹è±¡ï¼Œåˆ›å»ºæ–°å¯¹è±¡
            obj = SensorData()
            
        self.in_use_objects.add(obj)
        return obj
        
    def release(self, obj):
        """
        é‡Šæ”¾å¯¹è±¡å›æ± ä¸­
        """
        if obj in self.in_use_objects:
            self.in_use_objects.remove(obj)
            obj.reset()  # é‡ç½®å¯¹è±¡çŠ¶æ€
            self.available_objects.append(obj)
            
    def get_pool_status(self):
        """
        è·å–å¯¹è±¡æ± çŠ¶æ€
        """
        return {
            'available': len(self.available_objects),
            'in_use': len(self.in_use_objects),
            'total': len(self.available_objects) + len(self.in_use_objects)
        }

class SensorData:
    """
    ä¼ æ„Ÿå™¨æ•°æ®ç±»
    """
    def __init__(self):
        self.reset()
        
    def reset(self):
        """
        é‡ç½®å¯¹è±¡çŠ¶æ€
        """
        self.timestamp = None
        self.sensor_id = None
        self.temperature = 0.0
        self.humidity = 0.0
        self.solar_irradiance = 0.0
        self.power_output = 0.0
        
    def set_data(self, timestamp, sensor_id, temperature, 
                 humidity, solar_irradiance, power_output):
        """
        è®¾ç½®æ•°æ®
        """
        self.timestamp = timestamp
        self.sensor_id = sensor_id
        self.temperature = temperature
        self.humidity = humidity
        self.solar_irradiance = solar_irradiance
        self.power_output = power_output

# ä½¿ç”¨ç¤ºä¾‹
sensor_pool = SensorDataPool()

# å¤„ç†å¤§é‡ä¼ æ„Ÿå™¨æ•°æ®
for data_point in large_dataset:
    # ä»æ± ä¸­è·å–å¯¹è±¡
    sensor_obj = sensor_pool.acquire()
    
    # è®¾ç½®æ•°æ®
    sensor_obj.set_data(
        data_point['timestamp'],
        data_point['sensor_id'],
        data_point['temperature'],
        data_point['humidity'],
        data_point['solar_irradiance'],
        data_point['power_output']
    )
    
    # å¤„ç†æ•°æ®
    process_sensor_data(sensor_obj)
    
    # é‡Šæ”¾å¯¹è±¡å›æ± ä¸­
    sensor_pool.release(sensor_obj)
```

### å†…å­˜æ˜ å°„æ–‡ä»¶

```python
import mmap
import struct
import os

class MemoryMappedDataStore:
    """
    ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶å¤„ç†å¤§å‹æ•°æ®é›†
    """
    def __init__(self, filename, record_size=32, max_records=1000000):
        self.filename = filename
        self.record_size = record_size  # æ¯æ¡è®°å½•çš„å­—èŠ‚æ•°
        self.max_records = max_records
        self.file_size = record_size * max_records
        
        # åˆ›å»ºæˆ–æ‰“å¼€æ–‡ä»¶
        if not os.path.exists(filename):
            with open(filename, 'wb') as f:
                f.write(b'\x00' * self.file_size)
                
        self.file = open(filename, 'r+b')
        self.mmap = mmap.mmap(self.file.fileno(), 0)
        
    def write_record(self, index, timestamp, sensor_id, 
                    temperature, humidity, power):
        """
        å†™å…¥ä¸€æ¡è®°å½•
        """
        if index >= self.max_records:
            raise IndexError("Record index out of range")
            
        offset = index * self.record_size
        
        # æ‰“åŒ…æ•°æ®ï¼ˆä½¿ç”¨structæ ¼å¼ï¼‰
        data = struct.pack('dIffff', 
                          timestamp,    # double (8 bytes)
                          sensor_id,    # unsigned int (4 bytes)
                          temperature,  # float (4 bytes)
                          humidity,     # float (4 bytes)
                          power,        # float (4 bytes)
                          0.0)          # padding (4 bytes)
        
        self.mmap[offset:offset + self.record_size] = data
        
    def read_record(self, index):
        """
        è¯»å–ä¸€æ¡è®°å½•
        """
        if index >= self.max_records:
            raise IndexError("Record index out of range")
            
        offset = index * self.record_size
        data = self.mmap[offset:offset + self.record_size]
        
        # è§£åŒ…æ•°æ®
        timestamp, sensor_id, temperature, humidity, power, _ = \
            struct.unpack('dIffff', data)
            
        return {
            'timestamp': timestamp,
            'sensor_id': sensor_id,
            'temperature': temperature,
            'humidity': humidity,
            'power': power
        }
        
    def batch_read(self, start_index, count):
        """
        æ‰¹é‡è¯»å–è®°å½•
        """
        records = []
        for i in range(start_index, min(start_index + count, self.max_records)):
            records.append(self.read_record(i))
        return records
        
    def close(self):
        """
        å…³é—­æ–‡ä»¶
        """
        self.mmap.close()
        self.file.close()
        
    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

# ä½¿ç”¨ç¤ºä¾‹
with MemoryMappedDataStore('solar_data.dat') as store:
    # å†™å…¥æ•°æ®
    for i in range(10000):
        store.write_record(i, time.time(), i % 100, 
                          25.0 + i * 0.01, 60.0 + i * 0.001, 
                          1000 + i * 0.1)
    
    # è¯»å–æ•°æ®
    records = store.batch_read(0, 100)
    for record in records:
        print(f"Sensor {record['sensor_id']}: {record['power']}W")
```

## âš¡ å¹¶å‘å’Œå¼‚æ­¥ä¼˜åŒ–

### å¤šçº¿ç¨‹æ•°æ®å¤„ç†

```python
import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

class ConcurrentDataProcessor:
    """
    å¹¶å‘æ•°æ®å¤„ç†å™¨
    """
    def __init__(self, num_workers=4):
        self.num_workers = num_workers
        self.input_queue = queue.Queue()
        self.output_queue = queue.Queue()
        self.workers = []
        self.running = False
        
    def start(self):
        """
        å¯åŠ¨å·¥ä½œçº¿ç¨‹
        """
        self.running = True
        
        for i in range(self.num_workers):
            worker = threading.Thread(
                target=self._worker_thread,
                args=(i,),
                daemon=True
            )
            worker.start()
            self.workers.append(worker)
            
    def stop(self):
        """
        åœæ­¢å·¥ä½œçº¿ç¨‹
        """
        self.running = False
        
        # å‘é€åœæ­¢ä¿¡å·
        for _ in range(self.num_workers):
            self.input_queue.put(None)
            
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for worker in self.workers:
            worker.join()
            
    def _worker_thread(self, worker_id):
        """
        å·¥ä½œçº¿ç¨‹å‡½æ•°
        """
        while self.running:
            try:
                # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
                task = self.input_queue.get(timeout=1)
                
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                    
                # å¤„ç†ä»»åŠ¡
                result = self._process_task(task, worker_id)
                
                # å°†ç»“æœæ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
                self.output_queue.put(result)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.input_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"Worker {worker_id} error: {e}")
                
    def _process_task(self, task, worker_id):
        """
        å¤„ç†å•ä¸ªä»»åŠ¡
        """
        data = task['data']
        task_type = task['type']
        
        if task_type == 'power_analysis':
            return self._analyze_power_data(data, worker_id)
        elif task_type == 'efficiency_calculation':
            return self._calculate_efficiency(data, worker_id)
        elif task_type == 'anomaly_detection':
            return self._detect_anomalies(data, worker_id)
        else:
            return {'error': f'Unknown task type: {task_type}'}
            
    def _analyze_power_data(self, data, worker_id):
        """
        åˆ†æåŠŸç‡æ•°æ®
        """
        # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
        time.sleep(0.1)
        
        power_values = [point['power'] for point in data]
        
        return {
            'worker_id': worker_id,
            'task_type': 'power_analysis',
            'mean_power': sum(power_values) / len(power_values),
            'max_power': max(power_values),
            'min_power': min(power_values),
            'data_points': len(power_values)
        }
        
    def _calculate_efficiency(self, data, worker_id):
        """
        è®¡ç®—ç³»ç»Ÿæ•ˆç‡
        """
        # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
        time.sleep(0.05)
        
        total_input = sum(point['solar_irradiance'] for point in data)
        total_output = sum(point['power'] for point in data)
        
        efficiency = (total_output / total_input) * 100 if total_input > 0 else 0
        
        return {
            'worker_id': worker_id,
            'task_type': 'efficiency_calculation',
            'efficiency': efficiency,
            'total_input': total_input,
            'total_output': total_output
        }
        
    def _detect_anomalies(self, data, worker_id):
        """
        æ£€æµ‹å¼‚å¸¸æ•°æ®
        """
        # æ¨¡æ‹Ÿå¼‚å¸¸æ£€æµ‹ç®—æ³•
        time.sleep(0.08)
        
        power_values = [point['power'] for point in data]
        mean_power = sum(power_values) / len(power_values)
        
        anomalies = []
        for i, power in enumerate(power_values):
            if abs(power - mean_power) > mean_power * 0.5:  # 50%åå·®é˜ˆå€¼
                anomalies.append({
                    'index': i,
                    'power': power,
                    'deviation': abs(power - mean_power)
                })
                
        return {
            'worker_id': worker_id,
            'task_type': 'anomaly_detection',
            'anomalies_count': len(anomalies),
            'anomalies': anomalies,
            'mean_power': mean_power
        }
        
    def submit_task(self, task):
        """
        æäº¤ä»»åŠ¡
        """
        self.input_queue.put(task)
        
    def get_result(self, timeout=None):
        """
        è·å–å¤„ç†ç»“æœ
        """
        try:
            return self.output_queue.get(timeout=timeout)
        except queue.Empty:
            return None

# ä½¿ç”¨ç¤ºä¾‹
processor = ConcurrentDataProcessor(num_workers=4)
processor.start()

# æäº¤ä»»åŠ¡
tasks = [
    {'type': 'power_analysis', 'data': generate_sample_data(100)},
    {'type': 'efficiency_calculation', 'data': generate_sample_data(100)},
    {'type': 'anomaly_detection', 'data': generate_sample_data(100)}
]

for task in tasks:
    processor.submit_task(task)

# è·å–ç»“æœ
results = []
for _ in range(len(tasks)):
    result = processor.get_result(timeout=5)
    if result:
        results.append(result)
        
processor.stop()

# æ‰“å°ç»“æœ
for result in results:
    print(f"Task {result['task_type']} completed by worker {result['worker_id']}")
```

### å¼‚æ­¥I/Oä¼˜åŒ–

```python
import asyncio
import aiohttp
import aiofiles
import time
from typing import List, Dict

class AsyncDataCollector:
    """
    å¼‚æ­¥æ•°æ®æ”¶é›†å™¨
    """
    def __init__(self, max_concurrent=10):
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
    async def collect_sensor_data(self, sensor_urls: List[str]) -> List[Dict]:
        """
        å¼‚æ­¥æ”¶é›†å¤šä¸ªä¼ æ„Ÿå™¨çš„æ•°æ®
        """
        tasks = []
        
        for url in sensor_urls:
            task = asyncio.create_task(self._fetch_sensor_data(url))
            tasks.append(task)
            
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤å¼‚å¸¸ç»“æœ
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"Error fetching data from {sensor_urls[i]}: {result}")
            else:
                valid_results.append(result)
                
        return valid_results
        
    async def _fetch_sensor_data(self, url: str) -> Dict:
        """
        ä»å•ä¸ªä¼ æ„Ÿå™¨è·å–æ•°æ®
        """
        async with self.semaphore:  # é™åˆ¶å¹¶å‘æ•°
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url, timeout=5) as response:
                        if response.status == 200:
                            data = await response.json()
                            return {
                                'url': url,
                                'data': data,
                                'timestamp': time.time(),
                                'status': 'success'
                            }
                        else:
                            return {
                                'url': url,
                                'error': f'HTTP {response.status}',
                                'timestamp': time.time(),
                                'status': 'error'
                            }
            except Exception as e:
                return {
                    'url': url,
                    'error': str(e),
                    'timestamp': time.time(),
                    'status': 'error'
                }
                
    async def save_data_batch(self, data_batch: List[Dict], 
                             filename: str) -> bool:
        """
        å¼‚æ­¥æ‰¹é‡ä¿å­˜æ•°æ®
        """
        try:
            async with aiofiles.open(filename, 'w') as f:
                import json
                await f.write(json.dumps(data_batch, indent=2))
            return True
        except Exception as e:
            print(f"Error saving data: {e}")
            return False
            
    async def process_data_pipeline(self, sensor_urls: List[str], 
                                   output_file: str) -> Dict:
        """
        å®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“
        """
        start_time = time.time()
        
        # æ”¶é›†æ•°æ®
        print(f"Collecting data from {len(sensor_urls)} sensors...")
        sensor_data = await self.collect_sensor_data(sensor_urls)
        
        # å¤„ç†æ•°æ®
        print("Processing collected data...")
        processed_data = await self._process_collected_data(sensor_data)
        
        # ä¿å­˜æ•°æ®
        print(f"Saving data to {output_file}...")
        save_success = await self.save_data_batch(processed_data, output_file)
        
        end_time = time.time()
        
        return {
            'total_sensors': len(sensor_urls),
            'successful_collections': len([d for d in sensor_data if d.get('status') == 'success']),
            'processing_time': end_time - start_time,
            'save_success': save_success,
            'output_file': output_file
        }
        
    async def _process_collected_data(self, sensor_data: List[Dict]) -> List[Dict]:
        """
        å¤„ç†æ”¶é›†åˆ°çš„æ•°æ®
        """
        processed_data = []
        
        for sensor_result in sensor_data:
            if sensor_result.get('status') == 'success':
                data = sensor_result['data']
                
                # æ•°æ®å¤„ç†é€»è¾‘
                processed_item = {
                    'sensor_url': sensor_result['url'],
                    'timestamp': sensor_result['timestamp'],
                    'power': data.get('power', 0),
                    'efficiency': self._calculate_efficiency(data),
                    'status': 'processed'
                }
                
                processed_data.append(processed_item)
                
        return processed_data
        
    def _calculate_efficiency(self, data: Dict) -> float:
        """
        è®¡ç®—æ•ˆç‡
        """
        power = data.get('power', 0)
        irradiance = data.get('solar_irradiance', 1)
        
        return (power / irradiance) * 100 if irradiance > 0 else 0

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    collector = AsyncDataCollector(max_concurrent=5)
    
    # æ¨¡æ‹Ÿä¼ æ„Ÿå™¨URLåˆ—è¡¨
    sensor_urls = [
        f"http://sensor{i}.example.com/data" 
        for i in range(1, 21)  # 20ä¸ªä¼ æ„Ÿå™¨
    ]
    
    # æ‰§è¡Œæ•°æ®æ”¶é›†ç®¡é“
    result = await collector.process_data_pipeline(
        sensor_urls, 
        'collected_data.json'
    )
    
    print(f"Pipeline completed:")
    print(f"- Total sensors: {result['total_sensors']}")
    print(f"- Successful collections: {result['successful_collections']}")
    print(f"- Processing time: {result['processing_time']:.2f} seconds")
    print(f"- Save success: {result['save_success']}")

# è¿è¡Œå¼‚æ­¥ç¨‹åº
if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ“Š ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

### å¤šçº§ç¼“å­˜ç³»ç»Ÿ

```python
import time
import threading
from typing import Any, Optional, Dict
from collections import OrderedDict
import pickle
import redis

class MultiLevelCache:
    """
    å¤šçº§ç¼“å­˜ç³»ç»Ÿï¼šå†…å­˜ç¼“å­˜ -> Redisç¼“å­˜ -> æ•°æ®åº“
    """
    def __init__(self, 
                 memory_size=1000,
                 redis_host='localhost',
                 redis_port=6379,
                 redis_db=0):
        # L1ç¼“å­˜ï¼šå†…å­˜ç¼“å­˜ï¼ˆæœ€å¿«ï¼‰
        self.memory_cache = LRUCache(memory_size)
        
        # L2ç¼“å­˜ï¼šRedisç¼“å­˜ï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰
        try:
            self.redis_client = redis.Redis(
                host=redis_host, 
                port=redis_port, 
                db=redis_db,
                decode_responses=False
            )
            self.redis_available = True
        except:
            self.redis_client = None
            self.redis_available = False
            
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            'l1_hits': 0,
            'l1_misses': 0,
            'l2_hits': 0,
            'l2_misses': 0,
            'total_requests': 0
        }
        
        self.lock = threading.RLock()
        
    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜æ•°æ®
        """
        with self.lock:
            self.stats['total_requests'] += 1
            
            # L1ç¼“å­˜æŸ¥æ‰¾
            value = self.memory_cache.get(key)
            if value is not None:
                self.stats['l1_hits'] += 1
                return value
                
            self.stats['l1_misses'] += 1
            
            # L2ç¼“å­˜æŸ¥æ‰¾
            if self.redis_available:
                try:
                    redis_value = self.redis_client.get(key)
                    if redis_value is not None:
                        self.stats['l2_hits'] += 1
                        # ååºåˆ—åŒ–æ•°æ®
                        value = pickle.loads(redis_value)
                        # å›å¡«åˆ°L1ç¼“å­˜
                        self.memory_cache.put(key, value)
                        return value
                except Exception as e:
                    print(f"Redis error: {e}")
                    
            self.stats['l2_misses'] += 1
            return None
            
    def put(self, key: str, value: Any, ttl: int = 3600):
        """
        å­˜å‚¨æ•°æ®åˆ°ç¼“å­˜
        """
        with self.lock:
            # å­˜å‚¨åˆ°L1ç¼“å­˜
            self.memory_cache.put(key, value, ttl)
            
            # å­˜å‚¨åˆ°L2ç¼“å­˜
            if self.redis_available:
                try:
                    serialized_value = pickle.dumps(value)
                    self.redis_client.setex(key, ttl, serialized_value)
                except Exception as e:
                    print(f"Redis error: {e}")
                    
    def delete(self, key: str):
        """
        åˆ é™¤ç¼“å­˜æ•°æ®
        """
        with self.lock:
            # ä»L1ç¼“å­˜åˆ é™¤
            self.memory_cache.delete(key)
            
            # ä»L2ç¼“å­˜åˆ é™¤
            if self.redis_available:
                try:
                    self.redis_client.delete(key)
                except Exception as e:
                    print(f"Redis error: {e}")
                    
    def get_stats(self) -> Dict:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        """
        with self.lock:
            total_requests = self.stats['total_requests']
            if total_requests == 0:
                return self.stats
                
            l1_hit_rate = (self.stats['l1_hits'] / total_requests) * 100
            l2_hit_rate = (self.stats['l2_hits'] / total_requests) * 100
            overall_hit_rate = ((self.stats['l1_hits'] + self.stats['l2_hits']) / total_requests) * 100
            
            return {
                **self.stats,
                'l1_hit_rate': l1_hit_rate,
                'l2_hit_rate': l2_hit_rate,
                'overall_hit_rate': overall_hit_rate
            }

class LRUCache:
    """
    LRUï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ç¼“å­˜å®ç°
    """
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.cache = OrderedDict()
        self.expiry_times = {}
        
    def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å€¼
        """
        if key not in self.cache:
            return None
            
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if key in self.expiry_times:
            if time.time() > self.expiry_times[key]:
                self.delete(key)
                return None
                
        # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆæ ‡è®°ä¸ºæœ€è¿‘ä½¿ç”¨ï¼‰
        self.cache.move_to_end(key)
        return self.cache[key]
        
    def put(self, key: str, value: Any, ttl: int = 3600):
        """
        å­˜å‚¨ç¼“å­˜å€¼
        """
        if key in self.cache:
            # æ›´æ–°ç°æœ‰å€¼
            self.cache[key] = value
            self.cache.move_to_end(key)
        else:
            # æ·»åŠ æ–°å€¼
            if len(self.cache) >= self.capacity:
                # åˆ é™¤æœ€ä¹…æœªä½¿ç”¨çš„é¡¹
                oldest_key = next(iter(self.cache))
                self.delete(oldest_key)
                
            self.cache[key] = value
            
        # è®¾ç½®è¿‡æœŸæ—¶é—´
        if ttl > 0:
            self.expiry_times[key] = time.time() + ttl
            
    def delete(self, key: str):
        """
        åˆ é™¤ç¼“å­˜é¡¹
        """
        if key in self.cache:
            del self.cache[key]
        if key in self.expiry_times:
            del self.expiry_times[key]
            
    def clear_expired(self):
        """
        æ¸…ç†è¿‡æœŸé¡¹
        """
        current_time = time.time()
        expired_keys = [
            key for key, expiry_time in self.expiry_times.items()
            if current_time > expiry_time
        ]
        
        for key in expired_keys:
            self.delete(key)

# ç¼“å­˜è£…é¥°å™¨
def cached(cache_instance, ttl=3600, key_func=None):
    """
    ç¼“å­˜è£…é¥°å™¨
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                cache_key = f"{func.__name__}:{hash(str(args) + str(kwargs))}"
                
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache_instance.get(cache_key)
            if cached_result is not None:
                return cached_result
                
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache_instance.put(cache_key, result, ttl)
            
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
cache = MultiLevelCache(memory_size=500)

@cached(cache, ttl=1800, key_func=lambda sensor_id, date: f"power_analysis:{sensor_id}:{date}")
def analyze_sensor_power(sensor_id: str, date: str) -> Dict:
    """
    åˆ†æä¼ æ„Ÿå™¨åŠŸç‡æ•°æ®ï¼ˆæ¨¡æ‹Ÿè€—æ—¶æ“ä½œï¼‰
    """
    print(f"Analyzing power data for sensor {sensor_id} on {date}...")
    time.sleep(2)  # æ¨¡æ‹Ÿå¤æ‚è®¡ç®—
    
    return {
        'sensor_id': sensor_id,
        'date': date,
        'avg_power': 1250.5,
        'max_power': 1800.0,
        'min_power': 800.0,
        'efficiency': 85.2
    }

# æµ‹è¯•ç¼“å­˜æ•ˆæœ
start_time = time.time()
result1 = analyze_sensor_power("SENSOR_001", "2024-01-15")  # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ‰§è¡Œå‡½æ•°
print(f"First call took: {time.time() - start_time:.2f} seconds")

start_time = time.time()
result2 = analyze_sensor_power("SENSOR_001", "2024-01-15")  # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œä»ç¼“å­˜è·å–
print(f"Second call took: {time.time() - start_time:.2f} seconds")

# æ‰“å°ç¼“å­˜ç»Ÿè®¡
print("Cache stats:", cache.get_stats())
```

## ğŸ” æ€§èƒ½ç›‘æ§å’Œåˆ†æ

### æ€§èƒ½åˆ†æå™¨

```python
import cProfile
import pstats
import io
import time
import psutil
import threading
from functools import wraps
from typing import Dict, List, Callable

class PerformanceProfiler:
    """
    æ€§èƒ½åˆ†æå™¨
    """
    def __init__(self):
        self.profiles = {}
        self.system_stats = []
        self.monitoring = False
        self.monitor_thread = None
        
    def profile_function(self, func_name: str = None):
        """
        å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨
        """
        def decorator(func: Callable):
            @wraps(func)
            def wrapper(*args, **kwargs):
                name = func_name or func.__name__
                
                # åˆ›å»ºæ€§èƒ½åˆ†æå™¨
                profiler = cProfile.Profile()
                
                # å¼€å§‹åˆ†æ
                profiler.enable()
                start_time = time.time()
                
                try:
                    result = func(*args, **kwargs)
                finally:
                    # åœæ­¢åˆ†æ
                    profiler.disable()
                    end_time = time.time()
                    
                    # ä¿å­˜åˆ†æç»“æœ
                    self.profiles[name] = {
                        'profiler': profiler,
                        'execution_time': end_time - start_time,
                        'timestamp': time.time()
                    }
                    
                return result
            return wrapper
        return decorator
        
    def get_function_stats(self, func_name: str) -> str:
        """
        è·å–å‡½æ•°æ€§èƒ½ç»Ÿè®¡
        """
        if func_name not in self.profiles:
            return f"No profile data for function: {func_name}"
            
        profile_data = self.profiles[func_name]
        profiler = profile_data['profiler']
        
        # åˆ›å»ºå­—ç¬¦ä¸²ç¼“å†²åŒº
        s = io.StringIO()
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        stats = pstats.Stats(profiler, stream=s)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªå‡½æ•°
        
        # æ·»åŠ æ‰§è¡Œæ—¶é—´ä¿¡æ¯
        result = f"Function: {func_name}\n"
        result += f"Total execution time: {profile_data['execution_time']:.4f} seconds\n"
        result += f"Timestamp: {time.ctime(profile_data['timestamp'])}\n"
        result += "\n" + s.getvalue()
        
        return result
        
    def start_system_monitoring(self, interval: float = 1.0):
        """
        å¼€å§‹ç³»ç»Ÿç›‘æ§
        """
        if self.monitoring:
            return
            
        self.monitoring = True
        self.monitor_thread = threading.Thread(
            target=self._monitor_system,
            args=(interval,),
            daemon=True
        )
        self.monitor_thread.start()
        
    def stop_system_monitoring(self):
        """
        åœæ­¢ç³»ç»Ÿç›‘æ§
        """
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
            
    def _monitor_system(self, interval: float):
        """
        ç³»ç»Ÿç›‘æ§çº¿ç¨‹
        """
        while self.monitoring:
            try:
                # è·å–ç³»ç»Ÿä¿¡æ¯
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                # è·å–ç½‘ç»œä¿¡æ¯
                network = psutil.net_io_counters()
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                stats = {
                    'timestamp': time.time(),
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory.percent,
                    'memory_used_gb': memory.used / (1024**3),
                    'memory_total_gb': memory.total / (1024**3),
                    'disk_percent': (disk.used / disk.total) * 100,
                    'disk_used_gb': disk.used / (1024**3),
                    'disk_total_gb': disk.total / (1024**3),
                    'network_bytes_sent': network.bytes_sent,
                    'network_bytes_recv': network.bytes_recv
                }
                
                self.system_stats.append(stats)
                
                # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
                if len(self.system_stats) > 1000:
                    self.system_stats.pop(0)
                    
            except Exception as e:
                print(f"System monitoring error: {e}")
                
            time.sleep(interval)
            
    def get_system_stats_summary(self) -> Dict:
        """
        è·å–ç³»ç»Ÿç»Ÿè®¡æ‘˜è¦
        """
        if not self.system_stats:
            return {"error": "No system stats available"}
            
        # è®¡ç®—å¹³å‡å€¼
        cpu_values = [s['cpu_percent'] for s in self.system_stats]
        memory_values = [s['memory_percent'] for s in self.system_stats]
        
        return {
            'monitoring_duration': len(self.system_stats),
            'cpu_avg': sum(cpu_values) / len(cpu_values),
            'cpu_max': max(cpu_values),
            'cpu_min': min(cpu_values),
            'memory_avg': sum(memory_values) / len(memory_values),
            'memory_max': max(memory_values),
            'memory_min': min(memory_values),
            'latest_stats': self.system_stats[-1] if self.system_stats else None
        }

# ä½¿ç”¨ç¤ºä¾‹
profiler = PerformanceProfiler()

# å¼€å§‹ç³»ç»Ÿç›‘æ§
profiler.start_system_monitoring(interval=0.5)

@profiler.profile_function("solar_power_calculation")
def calculate_solar_power(irradiance_data: List[float], 
                         panel_efficiency: float) -> List[float]:
    """
    è®¡ç®—å¤ªé˜³èƒ½åŠŸç‡ï¼ˆæ¨¡æ‹Ÿå¤æ‚è®¡ç®—ï¼‰
    """
    power_output = []
    
    for irradiance in irradiance_data:
        # æ¨¡æ‹Ÿå¤æ‚çš„åŠŸç‡è®¡ç®—
        base_power = irradiance * panel_efficiency
        
        # æ·»åŠ æ¸©åº¦ä¿®æ­£ï¼ˆæ¨¡æ‹Ÿï¼‰
        temperature_factor = 0.95  # å‡è®¾æ¸©åº¦å½±å“
        corrected_power = base_power * temperature_factor
        
        # æ·»åŠ é˜´å½±å½±å“è®¡ç®—ï¼ˆæ¨¡æ‹Ÿï¼‰
        shadow_factor = 0.98
        final_power = corrected_power * shadow_factor
        
        power_output.append(final_power)
        
        # æ¨¡æ‹Ÿè®¡ç®—å»¶è¿Ÿ
        time.sleep(0.001)
        
    return power_output

@profiler.profile_function("data_analysis")
def analyze_power_data(power_data: List[float]) -> Dict:
    """
    åˆ†æåŠŸç‡æ•°æ®
    """
    if not power_data:
        return {}
        
    # åŸºæœ¬ç»Ÿè®¡
    total_power = sum(power_data)
    avg_power = total_power / len(power_data)
    max_power = max(power_data)
    min_power = min(power_data)
    
    # è®¡ç®—æ ‡å‡†å·®
    variance = sum((x - avg_power) ** 2 for x in power_data) / len(power_data)
    std_dev = variance ** 0.5
    
    # æŸ¥æ‰¾å³°å€¼
    peaks = []
    for i in range(1, len(power_data) - 1):
        if (power_data[i] > power_data[i-1] and 
            power_data[i] > power_data[i+1] and 
            power_data[i] > avg_power * 1.2):
            peaks.append(i)
            
    return {
        'total_power': total_power,
        'avg_power': avg_power,
        'max_power': max_power,
        'min_power': min_power,
        'std_dev': std_dev,
        'peak_count': len(peaks),
        'peak_indices': peaks
    }

# æµ‹è¯•æ€§èƒ½åˆ†æ
print("Running performance tests...")

# ç”Ÿæˆæµ‹è¯•æ•°æ®
test_irradiance = [800 + i * 10 for i in range(100)]

# æ‰§è¡Œè¢«åˆ†æçš„å‡½æ•°
power_results = calculate_solar_power(test_irradiance, 0.2)
analysis_results = analyze_power_data(power_results)

print(f"Analysis results: {analysis_results}")

# ç­‰å¾…ä¸€æ®µæ—¶é—´æ”¶é›†ç³»ç»Ÿç»Ÿè®¡
time.sleep(5)

# åœæ­¢ç³»ç»Ÿç›‘æ§
profiler.stop_system_monitoring()

# æ‰“å°æ€§èƒ½åˆ†æç»“æœ
print("\n" + "="*50)
print("PERFORMANCE ANALYSIS RESULTS")
print("="*50)

print(profiler.get_function_stats("solar_power_calculation"))
print("\n" + "-"*50)
print(profiler.get_function_stats("data_analysis"))

# æ‰“å°ç³»ç»Ÿç»Ÿè®¡æ‘˜è¦
print("\n" + "-"*50)
print("SYSTEM STATS SUMMARY")
print("-"*50)
system_summary = profiler.get_system_stats_summary()
for key, value in system_summary.items():
    if isinstance(value, float):
        print(f"{key}: {value:.2f}")
    else:
        print(f"{key}: {value}")
```

## ğŸ¯ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ–°èƒ½æºæ•°æ®å¤„ç†ç³»ç»Ÿä¼˜åŒ–

```python
class OptimizedEnergyDataSystem:
    """
    ä¼˜åŒ–çš„æ–°èƒ½æºæ•°æ®å¤„ç†ç³»ç»Ÿ
    """
    def __init__(self):
        # åˆå§‹åŒ–å„ç§ä¼˜åŒ–ç»„ä»¶
        self.cache = MultiLevelCache(memory_size=1000)
        self.profiler = PerformanceProfiler()
        self.data_processor = ConcurrentDataProcessor(num_workers=8)
        self.memory_store = MemoryMappedDataStore('energy_data.dat')
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        self.profiler.start_system_monitoring()
        self.data_processor.start()
        
    @profiler.profile_function("batch_process_sensors")
    def process_sensor_batch(self, sensor_data_batch: List[Dict]) -> Dict:
        """
        æ‰¹é‡å¤„ç†ä¼ æ„Ÿå™¨æ•°æ®
        """
        results = {
            'processed_count': 0,
            'total_power': 0,
            'avg_efficiency': 0,
            'anomalies': []
        }
        
        # å¹¶å‘å¤„ç†æ•°æ®
        tasks = []
        for i, sensor_data in enumerate(sensor_data_batch):
            task = {
                'type': 'power_analysis',
                'data': sensor_data,
                'batch_index': i
            }
            self.data_processor.submit_task(task)
            tasks.append(task)
            
        # æ”¶é›†ç»“æœ
        processed_results = []
        for _ in tasks:
            result = self.data_processor.get_result(timeout=10)
            if result:
                processed_results.append(result)
                
        # æ±‡æ€»ç»“æœ
        results['processed_count'] = len(processed_results)
        if processed_results:
            total_power = sum(r.get('mean_power', 0) for r in processed_results)
            results['total_power'] = total_power
            results['avg_power'] = total_power / len(processed_results)
            
        return results
        
    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        self.profiler.stop_system_monitoring()
        self.data_processor.stop()
        self.memory_store.close()

# æ€§èƒ½æµ‹è¯•
def run_performance_test():
    system = OptimizedEnergyDataSystem()
    
    try:
        # ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data = []
        for i in range(100):
            sensor_batch = []
            for j in range(50):  # æ¯æ‰¹50ä¸ªä¼ æ„Ÿå™¨
                sensor_batch.append({
                    'sensor_id': f'SENSOR_{i}_{j}',
                    'power': 1000 + j * 10,
                    'efficiency': 0.8 + j * 0.001,
                    'temperature': 25 + j * 0.1
                })
            test_data.append(sensor_batch)
            
        # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
        start_time = time.time()
        
        for batch in test_data:
            result = system.process_sensor_batch(batch)
            print(f"Processed batch: {result['processed_count']} sensors, "
                  f"Total power: {result['total_power']:.2f}W")
            
        end_time = time.time()
        
        print(f"\nTotal processing time: {end_time - start_time:.2f} seconds")
        print(f"Processed {len(test_data)} batches with {len(test_data[0])} sensors each")
        
        # æ‰“å°æ€§èƒ½ç»Ÿè®¡
        print("\nPerformance Analysis:")
        print(system.profiler.get_function_stats("batch_process_sensors"))
        
        print("\nSystem Stats:")
        stats = system.profiler.get_system_stats_summary()
        for key, value in stats.items():
            if isinstance(value, (int, float)):
                print(f"{key}: {value:.2f}")
                
    finally:
        system.cleanup()

if __name__ == "__main__":
    run_performance_test()
```

## ğŸ“š å­¦ä¹ èµ„æº

### æ¨èä¹¦ç±
- ã€Šé«˜æ€§èƒ½Pythonã€‹
- ã€ŠPythonæ€§èƒ½åˆ†æä¸ä¼˜åŒ–ã€‹
- ã€Šç³»ç»Ÿæ€§èƒ½è°ƒä¼˜ã€‹
- ã€Šå¹¶å‘ç¼–ç¨‹å®æˆ˜ã€‹

### åœ¨çº¿èµ„æº
- [Pythonæ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://docs.python.org/3/howto/perf_profiling.html)
- [ç³»ç»Ÿæ€§èƒ½ç›‘æ§å·¥å…·](https://github.com/giampaolo/psutil)
- [Redisç¼“å­˜æœ€ä½³å®è·µ](https://redis.io/documentation)

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼ŒéªŒè¯æ‚¨çš„æ€§èƒ½ä¼˜åŒ–æŠ€èƒ½ï¼š

- [ ] å®ç°ç®—æ³•æ—¶é—´å¤æ‚åº¦ä¼˜åŒ–
- [ ] åº”ç”¨å†…å­˜ç®¡ç†ä¼˜åŒ–æŠ€æœ¯
- [ ] ä½¿ç”¨å¹¶å‘å’Œå¼‚æ­¥ç¼–ç¨‹
- [ ] å®ç°å¤šçº§ç¼“å­˜ç³»ç»Ÿ
- [ ] è¿›è¡Œæ€§èƒ½åˆ†æå’Œç›‘æ§
- [ ] ä¼˜åŒ–I/Oæ“ä½œ
- [ ] å®ç°æµå¼æ•°æ®å¤„ç†
- [ ] åº”ç”¨ç³»ç»Ÿçº§ä¼˜åŒ–æŠ€æœ¯

## ğŸ‰ ä¸‹ä¸€æ­¥å­¦ä¹ 

å®Œæˆç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–åï¼Œæ‚¨å¯ä»¥ç»§ç»­å­¦ä¹ ï¼š

1. [åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡](/docs/tutorials/advanced/distributed-systems)
2. [å¾®æœåŠ¡æ¶æ„](/docs/tutorials/advanced/microservices)
3. [äº‘åŸç”Ÿå¼€å‘](/docs/tutorials/advanced/cloud-native)
4. [DevOpså®è·µ](/docs/tutorials/advanced/devops)

---

**éœ€è¦å¸®åŠ©ï¼Ÿ** [æŸ¥çœ‹å¸¸è§é—®é¢˜](/docs/resources/faq) æˆ– [åŠ å…¥æŠ€æœ¯è®¨è®º](/community)